BASELINE EXPLAINER-FC-EXPERT
    These results were taken from the paper "Explainable Automated Fact Checking for Public Health Claims" - https://aclanthology.org/2020.emnlp-main.623/

    BERTScore [F1-score]: 83.46
    ROUGE-1: 32.30, ROUGE-2: 13.46, ROUGE-L: 26.99
    SacreBLEU: 14.57

The following scores were taken from the "Justification Production.ipynb" notebook.

REBEL
    With Redundancy Removal
    BERTScore Precision: 0.7698669620769837, Recall: 0.7149430647322249, F1-score: 0.7400893391296876
    ROUGE: {'rouge1': 0.1852365614516192, 'rouge2': 0.04423259214688255, 'rougeL': 0.1383762726891537, 'rougeLsum': 0.13835729220665433}
    SacreBLEU: {'score': 0.6131975872914872, 'counts': [22547, 4054, 1153, 420], 'totals': [76698, 74246, 71794, 69342], 'precisions': [29.39711596130277, 5.460226813565714, 1.6059837869459843, 0.6056935190793459], 'bp': 0.17346947509517147, 'sys_len': 76698, 'ref_len': 211054}

    Without Redundancy Removal
    BERTScore Precision: 0.7594125878558073, Recall: 0.7160805000955479, F1-score: 0.7358768231328121
    ROUGE: {'rouge1': 0.18598747650926112, 'rouge2': 0.04174691768922138, 'rougeL': 0.14016746568197114, 'rougeLsum': 0.14018280050294787}
    SacreBLEU: {'score': 1.1458607457315961, 'counts': [28186, 4965, 1374, 489], 'totals': [107583, 105131, 102679, 100227], 'precisions': [26.19930658189491, 4.722679323891145, 1.3381509364134827, 0.48789248406118113], 'bp': 0.3822125743176673, 'sys_len': 107583, 'ref_len': 211054}

    With Random Evidence Retrieval
    BERTScore Precision: 0.7605961342668844, Recall: 0.7236838507029983, F1-score: 0.7405239717023019
    ROUGE: {'rouge1': 0.18712004578112668, 'rouge2': 0.04110963199581587, 'rougeL': 0.1346756159316954, 'rougeLsum': 0.13478027813597376}
    SacreBLEU: {'score': 0.7319894130561173, 'counts': [24314, 4205, 1116, 373], 'totals': [86574, 84122, 81670, 79218], 'precisions': [28.08464435049784, 4.998692375359597, 1.366474837761724, 0.4708525840086849], 'bp': 0.2374388694306979, 'sys_len': 86574, 'ref_len': 211054}

FRED
    BERTScore Precision: 0.7274315103323107, Recall: 0.7089957371870871, F1-score: 0.7163224876389023
    ROUGE: {'rouge1': 0.16236733149415855, 'rouge2': 0.03238380958423824, 'rougeL': 0.11764940465546127, 'rougeLsum': 0.11778244257033516}
    SacreBLEU: {'score': 1.260874829047713, 'counts': [32323, 4587, 1014, 351], 'totals': [217447, 215000, 212553, 210106], 'precisions': [14.864771645504423, 2.1334883720930233, 0.47705748683857674, 0.16705853235985646], 'bp': 1.0, 'sys_len': 217447, 'ref_len': 210943}

SPACY
    BERTScore Precision: 0.6378432206481355, Recall: 0.590168115025049, F1-score: 0.6122704977944469
    ROUGE: {'rouge1': 0.12394103993767025, 'rouge2': 0.010862979819433351, 'rougeL': 0.09206506107134355, 'rougeLsum': 0.09218885847482265}
    SacreBLEU: {'score': 0.03837888985277136, 'counts': [13446, 1237, 132, 34], 'totals': [47763, 45699, 43635, 41571], 'precisions': [28.151498021481064, 2.7068426004945403, 0.3025094534204194, 0.08178778475379471], 'bp': 0.03275249816276371, 'sys_len': 47763, 'ref_len': 211054}

LLAMA
    BERTScore Precision: 0.772799804126069, Recall: 0.7310317535238982, F1-score: 0.7501559526846421
    ROUGE: {'rouge1': 0.20037026037812533, 'rouge2': 0.04563264953539377, 'rougeL': 0.14449923746439158, 'rougeLsum': 0.14449086276230838}
    SacreBLEU: {'score': 0.6634781052886836, 'counts': [23649, 4268, 1309, 554], 'totals': [74653, 72201, 69749, 67297], 'precisions': [31.678566166128622, 5.911275467098793, 1.8767294154754908, 0.8232164881049675], 'bp': 0.1608740468555779, 'sys_len': 74653, 'ref_len': 211054}

CHATGPT
    BERTScore Precision: 0.7927515411376953, Recall: 0.7586347055435181, F1-score: 0.7732533633708953
    ROUGE: {'rouge1': 0.22254513407878912, 'rouge2': 0.07088603450777184, 'rougeL': 0.16965251558313799, 'rougeLsum': 0.16866709039655797}
    SacreBLEU: {'score': 2.8296757810403395, 'counts': [537, 155, 73, 46], 'totals': [2398, 2348, 2298, 2248], 'precisions': [22.39366138448707, 6.601362862010221, 3.176675369886858, 2.0462633451957295], 'bp': 0.5082336421339616, 'sys_len': 2398, 'ref_len': 4021}
